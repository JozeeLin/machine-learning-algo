# 第五章 决策树-part2

## 决策树的剪枝

**决策树生成算法递归地产生决策树**.以上决策树的生成算法很容易产生过拟合.解决这个问题的办法是考虑决策树的复杂度,对已生成的决策树进行简化.**决策树的简化过程称为剪枝**.

**决策树剪枝往往通过极小化决策树整体的损失函数或代价函数来实现**.

设树T的结点个数为|T|,t是树T的叶结点,该叶结点有$N_t$个样本点,其中k类的样本点有$N_{tk}$个,$k=1,2,...,K,H_t(T)$为叶结点t上的经验熵,$\alpha \geq 0$为参数,则决策树学习的损失函数可以定义为:
$$
C_\alpha(T) = \sum_{t=1}^{|T|}N_tH_t(T) +\alpha|T|
$$
其中经验熵为:
$$
H_t(T) = - \sum_k \frac{N_{tk}}{N_t}\log \frac{N_{tk}}{N_t}
$$
将经验熵公式代入损失函数式子中第一项,可得:
$$
C(T) = \sum_{t=1}^{|T|}N_tH_t(T) = - \sum_{t=1}^{|T|}\sum_{k=1}^{K} N_{tk} \log \frac{N_{tk}}{N_t}
$$
这时损失函数可以表示成:
$$
C_\alpha(T) = C(T) + \alpha|T|
$$
$C(T)$表示模型对训练数据的预测误差,即模型与训练数据的拟合程度,|T|表示模型复杂度,参数$\alpha \geq 0$控制两者之间的影响.较大的$\alpha$促使选择简单的模型,较小的$\alpha$促使选择较复杂的模型.$\alpha$为0意味着只考虑模型与训练数据之间的拟合程度,不考虑模型复杂度.

**剪枝,就是当$\alpha$确定时,选择损失函数最小的模型**.

> **损失函数的极小化**等价于**正则化的极大似然估计**.所以,利用损失函数最小原则进行剪枝就是用正则化的最大似然估计进行**模型选择**.

**剪枝算法流程:**

1. 计算每个节点的经验熵

2. 递归地从树的叶节点向上回缩

   设一组叶节点回缩到其父节点之前和之后的整体树分别为$T_B$和$T_A$,其对应的损失函数值分别是$C_\alpha(T_B)$与$C_\alpha(T_A)$,如果:

$$
C_\alpha(T_A) \leq C_\alpha(T_B)
$$

​	则进行剪枝,即将父节点变为新的叶节点.

1. 返回2,直到不能继续位置,得到损失函数最小的子树$T_\alpha$

> 决策树的剪枝算法可以由一种**动态规划**的算法实现.

## CART算法

分类与回归树(classification and regression tree, CART)模型.CART是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法(此条件概率分布为求解目标).

CART算法由以下两步组成:

1. 决策树生成:基于训练数据集生成决策树,生成的决策树要尽量大;
2. 决策树剪枝:用验证数据集对已生成的树进行剪枝并选择最优子树,这时用损失函数最小作为剪枝的标准.

### CART生成

决策树的生成就是递归的构建二叉决策树的过程,**对回归树用平方误差最小化准则,对分类树用基尼指数最小化准则,进行特征选择,生成二叉树**.

### 1.回归树的生成算法流程

输入:训练数据集D

输出:回归树$f(x)$

在训练数据集所在的输入空间中,递归的将每个区域划分为两个子区域并决定每个子区域熵的输出值,构建二叉决策树:

- 选择最优切分变量j与切分点s,求解
  $$
  \min_{j,s} [\min_{c_1} \sum_{x_i \in R_1(j,s)}(y_i-c_1)^2+\min_{c_2} \sum_{x_i \in R_2(j,s)}(y_i-c_2)^2]
  $$
  遍历变量j,对固定的切分变量j扫描切分点s,选择使上式达到最小值的(j,s)

- 用选定的对(j,s)划分区域并决定相应的输出值:
  $$
  R_1(j,s) = \{x|x^j \leq s \}, R_2(j,s)={x|x^j > s} \\
  \hat{c}_m = \frac{1}{N_m} \sum_{x_i \in R_m(j,s)} y_i, x \in R_m, m=1,2
  $$

- 继续对两个子区域调用步骤1,2,直至满足停止条件

- 将输入空间划分为M个区域$R_1,R_2,...,R_M$,生成决策树:
  $$
  f(x) = \sum_{m=1}^{M}\hat{c}_mI(x \in R_m)
  $$




### 基尼指数

分类问题中,假设有K个类,样本点属于第k类的概率为$p_k$,则概率分布的基尼指数定义为:
$$
\rm{Gini}(p) = \sum_{k=1}^K p_k(1-p_k) = 1-\sum_{k=1}^{K}p_k^2
$$
对于给定的样本集合D,其基尼指数为:
$$
\rm{Gini}(D) = 1-\sum_{k=1}^K (\frac{|C_k|}{|D|})^2
$$
这里,$C_k$是D中属于第k类的样本子集,K是类的个数.

如果样本集合D根据特征A是否取某一可能值a,被分割成$D_1$和$D_2$两部分,即:
$$
D_1 = \{(x,y) \in D | A(x)=a\}, D_2=D-D_1
$$
则在特征A的条件下,集合D的基尼指数定义为:
$$
\rm{Gini}(D,A) = \frac{|D_1|}{|D|}\rm{Gini}(D_1)+\frac{|D_2|}{|D|}\rm{Gini}(D_2)
$$
基尼指数Gini(D)表示集合D的不确定性,基尼指数Gini(D,A)表示经A=a分割后集合D的不确定性.基尼指数值越大,样本集合的不确定性也就越大,这一点与熵相似.



### 2.分类树的生成算法流程

输入:训练数据集D,停止计算的条件

输出:CART决策树

1.设节点的训练数据集为D,计算现有特征对数据集的基尼指数.次数,对每一个特征A,对其可能取的每个值a,根据样本点对A=a的测试为"是"或"否"将D分割成$D_1$和$D_2$两部分,计算A=a时的基尼指数.

2.在所有可能的特征A以及它们所有可能的切分点a中,选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点.依最优特征与最优切分点,从现结点生成两个子结点,将训练数据集依特征分配到两个子结点中去.

3.对两个子结点递归地调用1,2,直至满足停止条件

4.生成CART决策树

算法停止计算的条件是结点中的样本个数小于预定阈值,或样本集的基尼指数小于预定阈值(样本基本属于同一类),或者没有更多特征.

### CART剪枝

剪枝算法流程:

1.设k=0,T=$T_0$

2.设$\alpha=+\infty$

3.自下而上地对各内部结点t计算$C(T_t)$,$|T_t|$以及
$$
g(t) = \frac{C(t) - C(T_t)}{|T_t|-1}
$$
这里,$T_t$表示以t为根结点的子树,$C_(T_t)$是对训练数据的预测误差,$|T_t|$是$T_t$的叶节点个数.

4.对$g(t) = \alpha$ 的内部结点t进行剪枝,并对叶节点t以多数投票法决定其类别,得到树T

5.设k=k+1,$\alpha_k = \alpha, T_k = T$

6.如果$T_k$不是由根结点及两个叶结点构成的树,则回到步骤3,否则令$T_k=T_n$

7.采用交叉验证法在子树序列$T_0,T_1,...,T_n$中选取最优子树$T_\alpha$