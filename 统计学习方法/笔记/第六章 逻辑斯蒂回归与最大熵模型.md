# 第六章 逻辑斯蒂回归与最大熵模型

逻辑斯蒂回归模型与最大熵模型都属于对数线性模型.

## 逻辑斯蒂回归模型

### 逻辑斯蒂分布

设X是连续随机变量,X服从逻辑斯蒂分布是指X具有下列的分布函数和密度函数:
$$
F(x) = P(X \leq x) = \frac{1}{1+e^{-(x-\mu)/\gamma}} \\
f(x) = F^{'}(x) = \frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^2}
$$
式中,$\mu$为位置参数,$\gamma > 0$为形状参数.

分布函数,以点$(\mu,\frac{1}{2})$处对称:
$$
F(-x+\mu) - \frac{1}{2} = -F(x+\mu)+\frac{1}{2}
$$

### 二项逻辑斯蒂回归模型

二项逻辑斯蒂回归模型由条件概率分布$P(Y|X)$表示,形式为**参数化的逻辑斯蒂分布**.

条件概率分布$P(Y|X)$表示如下:
$$
P(Y=1|x) = \frac{\exp(w.x+b)}{1+\exp(w.x+b)} \\
P(Y=0|x) = \frac{1}{1+\exp(w.x+b)}
$$
这里,$x \in \R^n,Y \in \{0,1\},w \in \R^n, b \in \R$,w称为权重,b称为偏置,w.x表示w和x的内积.

一个事件发生的几率表示成:
$$
\rm{logit}(p) = \log \frac{p}{1-p}
$$
把二项逻辑斯蒂回归模型的条件概率分布函数代入上式,得:
$$
\log \frac{P(Y=1|x)}{1-P(Y=1|x)} = w.x+b
$$
反之,也就是说可以**使用条件概率分布来把线性函数w.x+b转换成概率**,这样的话,两个函数之间的关系为:

**线性函数的值越接近正无穷,概率值就越接近1;线性函数的值越接近负无穷,概率值就越接近0**.

### 模型参数估计

**使用极大似然估计法估计模型参数**.

设:$P(Y=1|x) = \pi(x), P(Y=0|x)=1-\pi(x)$

似然函数表示为:
$$
\prod_{i=1}^{N} [\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}
$$
对数似然函数表示为:
$$
\begin{aligned}
L(w) &= \sum_{i=1}^{N} [y_i \log \pi(x_i)+(1-y_i)\log(1-\pi(x_i))] \\
&= \sum_{i=1}^{N} [y_i \log \frac{\pi(x_i)}{1-\pi(x_i)}+\log(1-\pi(x_i))] \\
&= \sum_{i=1}^{N} [y_i(w.x)-\log (1+\exp(w.x_i))]
\end{aligned}
$$
这样问题就变成了以对数似然函数为目标函数的最优化问题.

### 多项逻辑斯蒂回归

多项逻辑斯蒂回归的模型是:
$$
\begin{aligned}
P(Y=k|x) &= \frac{\exp(w_k.x)}{1+\sum_{k=1}^{K-1}\exp(w_k.x)} , k=1,2,...,K-1 \\
P(Y=K|x) &= \frac{1}{1+\sum_{k=1}^{K-1}\exp(w_k.x)}
\end{aligned}
$$
这里,$x \in \R^{n+1},w_k \in \R^{n+1}$

>  从二项逻辑斯蒂回归推广到多项逻辑斯蒂回归,表示从二分类问题推广到多分类问题.

## 最大熵模型

### 最大熵原理

最大熵原理是概率模型学习的一个准则.

最大熵原理认为,学习概率模型时,在所有可能的概率模型(分布)中,熵最大的模型是最好的模型,通常用约束条件来确定概率模型的集合.所以,**最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型**.

根据熵的定义,我们可以知道,**当X服从均匀分布时,熵最大**.

### 最大熵模型的定义

> 关于约束条件的描述请参阅<统计学习方法>这本书.

假设满足所有约束条件的模型集合为:
$$
C \equiv \{P \in \mathcal{P} | E_P(f_i) = E_{\widetilde{P}}(f_i), i=1,2,...,n\}
$$
定义在条件概率分布$P(Y|X)$上的条件熵为:
$$
H(P) = - \sum_{x,y} \widetilde{P}(x)P(y|x)\log P(y|x)
$$
则模型集合C中条件熵H(P)最大的模型称为最大熵模型,式中的对数为自然对数.

### 最大熵模型的学习

最大熵模型的学习过程就是求解最大熵模型的过程.最大熵模型的学习可以形式化为**约束最优化问题**.

对于给定的训练数据集T,以及特征函数$f_i(x,y) , i=1,2,...,n$,最大熵模型的学习等价于约束最优化问题:
$$
\begin{aligned}
\max_{P \in C} \ &H(P) = -\sum_{x,y} \widetilde{P}(x)P(y|x)\log P(y|x) \\
\rm{s.t.} \ & E_P(f_i) = E_{\widetilde{P}} (f_i) , i=1,2,...,n \\
& \sum_yP(y|x) = 1
\end{aligned}
$$
按照最优化问题的习惯,将求最大值问题改写为等价的求最小值问题:
$$
\begin{aligned}
\min_{P \in C} \ &-H(P) = \sum_{x,y} \widetilde{P}(x)P(y|x)\log P(y|x) \\
\rm{s.t.} \ & E_P(f_i) - E_{\widetilde{P}} (f_i) = 0 , i=1,2,...,n \\
& \sum_yP(y|x) = 1
\end{aligned}
$$
将上式的**约束最优化的原始问题转换为无约束最优化的对偶问题**:

> 又是一个通过求解对偶问题来间接求解原始问题.问题的简化

首先,引入拉格朗日乘子(通常用于解决约束最优化问题),$w_0,w_1,...,w_n$,定义拉格朗日函数$L(P,w)$:
$$
\begin{aligned}
L(P,w) \equiv &-H(P) + w_0(1-\sum_yP(y|x)) + \sum_{i=1}^n w_i(E_{\widetilde{P}}(f_i)-E_P(f_i)) \\
= &\sum_{x,y}\widetilde{P}(x)P(y|x)\log P(y|x)+w_0(1-\sum_yP(y|x)) \\
&+\sum_{i=1}^nw_i(\sum_{x,y}\widetilde{P}(x,y)f_i(x,y)-\sum_{x,y}\widetilde{P}(x)P(y|x)f_i(x,y))
\end{aligned}
$$
最优化的原始问题是:
$$
\min_{P \in C} \max_{w} L(P,w) 
$$
对偶问题是:
$$
\max_w \min_{P \in C} L(P,w)
$$
首先,求解对偶问题内部的极小化问题,它是P的函数,此时的w为常数:

通过令偏导数为0来求解内部最小化问题:
$$
\begin{aligned}
\frac{\partial{L(P,w)}}{\partial{P(y|x)}} &= \sum_{x,y} \widetilde{P}(x)(\log P(y|x)+1) - \sum_y w_0 - \sum_{x,y}(\widetilde{P}(x)\sum_{i=1}^nw_if_i(x,y)) \\
&= \sum_{x,y}\widetilde{P}(x)(\log P(y|x)+1-w_0-\sum_{i=1}^nw_if_i(x,y))
\end{aligned}
$$
令上式的为0,求得:
$$
P(y|x) = \exp(\sum_{i=1}^nw_if_i(x,y)+w_0-1) = \frac{\exp(\sum_{i=1}^nw_if_i(x,y))}{\exp(1-w_0)}
$$
由于$\sum_yP(y|x) = 1$,得:
$$
P_w(y|x) = \frac{1}{Z_w(x)} \exp(\sum_{i=1}^n w_i f_i(x,y))
$$
其中,
$$
Z_w(x) = \sum_y \exp (\sum_{i=1}^nw_i f_i(x,y))
$$
$Z_w(x)$称为规范化因子;$f_i(x,y)$是特征函数;$P_w=P_w(y|x)$就是最大熵模型.

### 极大似然估计

**对偶函数的极大化等价于最大熵模型的极大似然估计**.

已知训练数据的经验概率分布$\widetilde{P}(X,Y)$,条件概率分布$P(Y|X)$的对数似然函数表示为(参考"**模型参数估计**"):
$$
L_{\widetilde{P}}(P_w) = \log \prod_{x,y}P(y|x)^{\widetilde{P}(x,y)} = \sum_{x,y}\widetilde{P}(x,y)\log P(y|x)
$$

## 模型学习的最优化算法

逻辑斯蒂回归模型,最大熵模型学习归结为以似然函数为**目标函数的最优化问题**,通常**通过迭代算法求解**.

### 改进的迭代尺度法

改进的迭代尺度法是一种最大熵模型学习的最优化算法.

已知最大熵模型为:
$$
P_w(y|x) = \frac{1}{Z_w(x)} \exp (\sum_{i=1}^n w_i f_i(x,y))
$$
其中,
$$
Z_w(x) = \sum_y \exp (\sum_{i=1}^n w_i f_i(x,y))
$$
对数似然函数为:
$$
L(w) = \sum_{x,y} \widetilde{P}(x,y)\sum_{i=1}^nw_if_i(x,y) - \sum_x \widetilde{P}(x)\log Z_w(x)
$$
目标是通过极大似然估计学习模型参数,即求对数似然函数的极大值$\hat{w}$

