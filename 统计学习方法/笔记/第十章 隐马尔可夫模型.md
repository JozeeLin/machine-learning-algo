# 隐马尔可夫模型

隐马尔可夫模型(hidden Markov model, HMM)是可用于标注问题的统计学习模型,属于生成模型.

## 隐马尔可夫模型的基本概念

### 定义

隐马尔可夫模型是**关于时序的概率模型**,描述由一个隐藏的马尔科夫链随机生成不可观测的状态随机序列,再由各个状态生成一个观测而产生观测随机序列的过程.这里包含两个序列,分别为状态序列和观测序列.序列的每一个位置又可以看作是一个时刻.

隐马尔可夫模型由初始概率分布,状态转移概率分布以及观测概率分布确定,隐马尔可夫模型的形式定义如下:

设Q是所有可能的状态的集合,V是所有可能的观测集合,
$$
Q = \{q_1,q_2,...,q_N\} \ , V=\{v_1,v_2,...,v_N\}
$$
其中,N是可能的状态数,M是可能的观测数.

I是长度为T的状态序列,O是对应的观测序列,
$$
I = (i_1,i_2,...,i_T) \ , O = (o_1,o_2,...,o_T)
$$
A是状态转移概率矩阵:
$$
A = [a_{ij}]_{N\rm{x}N}
$$
其中,
$$
a_{ij} = P(i_{t+1}=q_j|i_t=q_i)\ , i=1,2,...,N \ ; j=1,2,...,N
$$
表示在时刻t处于状态$q_i$的条件下在时刻t+1转移到状态$q_j$的概率.

B是观测概率矩阵:
$$
B = [b_j(k)]_{N\rm{x}N}
$$
其中,
$$
b_j(k) = P(o_t=v_k|i_t=q_j)\ , k=1,2,...,M \ ; j=1,2,...,N
$$
表示在时刻t处于状态$q_j$的条件下,生成观测$v_k$的概率.

$\pi$是初始状态概率向量:
$$
\pi = (\pi_i)
$$
其中,
$$
\pi_i = P(i_1=q_i) \ , i=1,2,...,N
$$
是时刻t=1处于状态$q_i$的概率.

**隐马尔可夫模型由初始状态概率向量$\pi$,状态转移概率矩阵A和观测概率矩阵B决定**.$\pi$和A决定状态序列,B决定观测序列.因此,隐马尔可夫模型$\lambda$可以用三元符号表示,即
$$
\lambda = (A,B,\pi)
$$
**A,B,$\pi$称为隐马尔可夫模型的三要素**.

### 两个基本假设

1. 齐次马尔可夫性假设,即假设隐藏的马尔科夫链在任意时刻t的状态只依赖于其前一时刻的状态,与其他时刻的状态及观测无关,也与时刻t无关
   $$
   P(i_t|i_{t-1},o_{t-1},...,i_1,o_1)  = P(i_t|i_{t-1}) \ , t=1,2,...,T
   $$

2. 观测独立性假设,即假设任意时刻的观测只依赖于该时刻的马尔科夫链的状态,与其他观测即状态无关
   $$
   P(o_t|i_T,o_T,i_{T-1},o_{T-1},...,i_{t+1},o_{t+1},i_t,o_t,...,i_1,o_1) = P(o_t|i_t)
   $$




### 观测序列的生成过程

根据隐马尔可夫模型定义,可以将一个长度为T的观测序列$O=(o_1,o_2,...,o_T)$的生成过程描述如下:

输入:隐马尔可夫模型$\lambda=(o_1,o_2,...,o_T)$,观测序列长度T;

输出:观测序列$O=(o_1,o_2,...,o_T)$

1. 按照初始状态分布$\pi$产生状态$i_1$
2. 令t=1
3. 按照状态$i_t$的观测概率分布$b_{i_t}(k)$生成$o_t$
4. 按照状态$i_t$的状态转移概率分布$\{a_{{i_ti_{t+1}}}\}$产生状态$i_{t+1}$,$i_{i+1}=1,2,..,N$
5. 令t=t+1;如果t<T,转步3,否则,停止

### 隐马尔可夫模型的3个基本问题

1. 概率计算问题.给定模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,...,o_T)$,**计算在模型$\lambda$下观测序列O出现的概率$P(O|\lambda)$**
2. 学习问题.已知观测序列$O=(o_1,o_2,...,o_T)$,估计模型$\lambda=(A,B,\pi)$参数,使得在该模型下观测序列概率$P(O|\lambda)$最大.**即用极大似然估计的方法估计参数.**
3. 预测问题,也称为**解码问题**.已知模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,...,o_T)$,求给定观测序列条件概$P(I|O)$最大的状态序列$I=(i_1,i_2,...,i_T)$.**即给定观测序列,求最有可能的对应的状态序列**.

## 概率计算算法

###直接计算法

**在概念上可行但计算上不可行的直接计算法**.

输入:模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,...,o_T)$

输出:观测序列O出现的概率$P(O|\lambda)$

1. 枚举所有可能的长度为T的状态序列I
2. 求各个状态序列I与观测序列O的联合概率$P(O,I|\lambda)$
3. 对所有可能的状态序列求和,得到$P(O|\lambda)$

状态序列$I=(i_1,i_2,...,i_T)$的概率是:
$$
P(I|\lambda) = \pi_{i_1}a_{i_1i_2}a_{i_2i_3}...a_{i_{r-1}i_r}
$$
对固定的状态序列$I=(i_1,i_2,...,i_T)$,观测序列$O=(o_1,o_2,...,o_T)$的概率是:
$$
P(O|I,\lambda) = b_{i_1}(o_1)b_{i_2}(o_2)...b_{i_r}(o_r)
$$
O和I同时出现的联合概率为:
$$
P(O,I|\lambda) = P(O|I,\lambda)P(I|\lambda)
$$
然后,求和,得到$P(O|\lambda)$:
$$
P(O|\lambda) = \sum_I P(O|I,\lambda)P(I|\lambda)
$$
以上公式计算量很大,是$O(TN^T)$

### 前向算法(动态规划)

**前向概率**:给定隐马尔可夫模型$\lambda$,定义到时刻t部分观测序列为$o_1,o_2,...,o_t$且状态为$q_i$的概率为前向概率,记作:
$$
\alpha_t(i) = P(o_1,o_2,...,o_t,i_t=q_i|\lambda)
$$
可以递推地求得前向概率$\alpha_t(i)$及观测序列概率$P(O|\lambda)$

**观测序列概率的前向算法流程**:

输入:隐马尔可夫模型$\lambda$,观测序列O

输出:观测序列概率$P(O|\lambda)$

1. 初值
   $$
   \alpha_1(i) = \pi_i b_i(o_1) \ , i=1,2,...,N
   $$

2. 递推 对t=1,2,...,T-1
   $$
   \alpha_{t+1}(i) = \left[ \sum_{j=1}^{N} \alpha_t(j) a_{ji} \right] b_i(o_{t+1}) \ , i=1,2,...,N
   $$

3. 终止
   $$
   P(O|\lambda) = \sum_{i=1}^N \alpha_T(i)
   $$




步骤一:初始化前向概率,表示初始时刻的状态$i_1=q_i$和观测$o_1$的联合概率

步骤二:是前向概率的递推公式,计算到时刻t+1部分观测序列为$o_1,o_2,...,o_t,o_{t+1}$且在时刻t+1处于状态$q_i$的前向概率.

步骤三:给出了观测序列概率的计算公式

**前向算法实际是基于"状态序列的路径结构"递推计算$P(O|\lambda)$的算法**.前向算法高效的关键在于**计算局部前向概率,然后利用路径结构将前向概率"递推"到全局**.(子问题推出原问题的DP思想).**前向算法的复杂度为$O(TN^2)$**.

### 后向算法

**后向概率**:给定隐马尔可夫模型$\lambda$,定义在时刻t状态为$q_i$的条件下,从t+1到T的部分观测序列为$o_{t+1},o_{t+2},...,o_T$的概率为后向概率,记作:
$$
\beta_t(i) = P(o_{t+1},o_{t+2},...,o_T|i_t=q_i,\lambda)
$$
可以用递推的方法求得后向概率$\beta_t(i)$及观测序列概率$P(O|\lambda)$

**观测序列概率的后向算法:**

输入:隐马尔可夫模型$\lambda$,观测序列O

输出:观测序列概率

1. $$
   \beta_T(i) = 1\ , i=1,2,...,N
   $$

2. 对t=T-1,T-2,...,1
   $$
   \beta_t(i) = \sum_{j=1}^N a_{ij}b_j(o_{t+1})\beta_{t+1}(j) \ , i=1,2,...,N
   $$

3. $$
   P(O|\lambda) = \sum_{i=1}^N \pi_i b_i(o_1)\beta_1(i)
   $$



步骤1:初始化后向概率,对最终时刻的所有状态$q_i$规定$\beta_T(i)=1$

步骤2:后向概率的递推公式.计算在时刻t状态为$q_i$条件下时刻t+1之后的观测序列为$o_{t+1},o_{t+2},...,o_T$的后向概率$\beta_t(i)$.

步骤3:求$P(O|\lambda)$

> 利用前向概率和后向概率的定义可以将观测序列概率$P(O|\lambda)$统一写成:
> $$
> P(O|\lambda) = \sum_{i=1}^N\sum_{j=1}^N \alpha_i(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j) \ , t=1,2,...,T-1
> $$
> 此式当t=1和t=T-1分别对应后向算法和前向算法的概率公式.

## 学习算法

### 监督学习方法

假设已给训练数据包含S个长度相同的**观测序列**和对应的**状态序列**$\{(O_1,I_1),(O_2,I_2),...,(O_S,I_S)\}$,那么可以利用**极大似然估计法**来估计隐马尔可夫模型的参数.具体方法如下:

1. 转移概率$a_{ij}$的估计

   设样本中时刻t处于状态i时刻t+1转移到状态j的频数为$A_{ij}$,那么状态转移概率$a_{ij}$的估计是:
   $$
   \hat{a}_{ij} = \frac{A_{ij}}{\sum_{j=1}^{N}A_{ij}} \ , i=1,2,...,N \ ; j=1,2,...,N
   $$

2. 观测概率$b_j(k)$的估计

   设样本中状态为j并观测为k的频数是$B_{jk}$,那么状态j观测为k的概率$b_j(k)$的估计是:
   $$
   \hat{b}_j(k) = \frac{B_{jk}}{\sum_{k=1}^M B_{jk}} \ , j=1,2,...,N \ ; k=1,2,...,M
   $$

3. 初始状态概率$\pi_i$的估计$\hat{\pi}_i$为S个样本中初始状态为$q_i$的频率

### 非监督学习方法(Baum-Welch算法)

假设给定训练数据只包含S个长度为T的观测序列$\{O_1,O_2,...,O_S\}$而没有对应的状态序列,目标是学习隐马尔可夫模型$\lambda=(A,B,\pi)$的参数.**将观测序列数据看作观测数据O,状态序列数据看作不可观测的隐数据I,那么隐马尔可夫模型实际上是一个含有隐变量的概率模型**:
$$
P(O|\lambda) = \sum_I P(O|I,\lambda)P(I|\lambda)
$$
它的**参数学习可以由EM算法实现**.

1. **确定完全数据的对数似然函数**

   所有观测数据写成$O=(o_1,o_2,...,o_T)$,所有隐数据写成$I = (i_1,i_2,...,i_T) $,完全数据是$(O,I)=(o_1,o_2,...,o_T,i_1,i_2,...,i_T)$.完全数据的对数似然函数是$\log P(O,I|\lambda)$

2. EM算法的E步:**求Q函数**:
   $$
   Q(\lambda,\widetilde{\lambda}) = \sum_I \log P(O,I|\lambda) P(O,I|\widetilde{\lambda})
   $$
   其中,$\widetilde{\lambda}$是隐马尔可夫模型参数的当前估计值,$\lambda$是要极大化的隐马尔可夫模型的参数.
   $$
   P(O,I|\lambda) = \pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}i_T}b_{i_T}(o_T)
   $$
   于是,Q函数可以写成:
   $$
   \begin{aligned}
   Q(\lambda,\widetilde{\lambda}) &= \sum_I \log \pi_{i_1} P(O,I|\widetilde{\lambda}) \\
   &+ \sum_I \left( \sum_{t=1}^{T-1} \log a_{i_ti_{t+1}} \right) P(O,I|\widetilde{\lambda}) +\sum_I \left( \sum_{t=1}^T \log b_{i_t}(o_t) \right)P(O,I|\widetilde{\lambda}) 
   \end{aligned}
   $$
   式中求和都是对所有训练数据的序列总长度T进行的.

3. EM算法的M步:**极大化Q函数$Q(\lambda,\widetilde{\lambda})$求模型参数A,B,$\pi$**

   由于要极大化的参数单独出现在3个项中,所以只需对各项分别极大化.

   1. 第一项写成:
      $$
      \sum_I \log \pi_{i_1} P(O,I|\widetilde{\lambda}) = \sum_{i=1}^N \log \pi_i P(O,i_1=i|\widetilde{\lambda})
      $$
      注意到$\pi_i$满足约束条件$\sum_{i=1}^N \pi_i=1$,利用拉格朗日乘子法,写出**拉格朗日函数**:
      $$
       \sum_{i=1}^N \log \pi_i P(O,i_1=i|\widetilde{\lambda}) + \gamma \left( \sum_{i=1}^N \pi_i - 1 \right)
      $$
      对其求偏导数并令结果为0,得:
      $$
      P(O,i_1=i|\widetilde{\lambda}) + \gamma \pi_i = 0
      $$
      对i求和得到$\gamma$:
      $$
      \gamma = - P(O|\widetilde{\lambda})
      $$
      代入偏导数为0的公式中,得到:
      $$
      \pi_i = \frac{P(O,i_1=i|\widetilde{\lambda}) }{P(O|\widetilde{\lambda})}
      $$

   2. 第二项写成:
      $$
      \sum_I \left( \sum_{t=1}^{T-1} \log a_{i_ti_{t+1}} \right) P(O,I|\widetilde{\lambda})  = \sum_{i=1}^N \sum_{j=1}^N \sum_{t=1}^{T-1} \log a_{ij} P(O,i_t=i,i_{t+1}=j|\widetilde{\lambda})
      $$
      存在约束条件$\sum_{j=1}^N a_{ij}=1$ ,写出**拉格朗日函数**,求得:
      $$
      a_{ij} = \frac{\sum_{t=1}^{T-1}P(O,i_t=i,i_{t+1}=j|\widetilde{\lambda})}{\sum_{t=1}^{T-1}P(O,i_t=i|\widetilde{\lambda})}
      $$

   3. 第三项写成:
      $$
      \sum_I \left( \sum_{t=1}^T \log b_{i_t}(o_t) \right)P(O,I|\widetilde{\lambda})  = \sum_{j=1}^N \sum_{t=1}^T \log b_j(o_t)P(O,i_t=j|\widetilde{\lambda})
      $$
      存在约束条件$\sum_{k=1}^M b_j(k)=1$,注意,只有在$o_t=v_k$时,$b_j(o_t)$对$b_j(k)$的偏导数才不为0,以$I(o_t=v_k)$表示,求得:
      $$
      b_j(k) = \frac{\sum_{t=1}^TP(O,i_t=j|\widetilde{\lambda})I(o_t=v_k)}{\sum_{t=1}^TP(O,i_t=j|\widetilde{\lambda})}
      $$



   **Baum-Welch算法是EM算法在隐马尔可夫模型学习中的具体实现**.

   **Baum-Welch算法流程:**

   <参阅统计学习方法P183>

   ##预测算法

### 近似算法

近似算法的想法是,在每个时刻t选择在该时刻**最可能出现的状态$i_t^*$,**从而**得到一个状态序列**$I^*=(i_1^*,i_2^*,...,i_T^*)$,将它**作为预测的结果**.

给定隐马尔可夫模型$\lambda$和观测序列O,在时刻t处于状态$q_i$的概率$\gamma_t(i)$是:
$$
\gamma_t(i) = \frac{\alpha_t(i)\beta_t(i)}{P(O|\lambda)} = \frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N\alpha_t(j)\beta_t(j)}
$$
在每一时刻t最有可能的状态$i_t^*$是:
$$
i_t^* = \arg \max_{1\leq i\leq N}[\gamma_t(i)] \ , t=1,2,...,T
$$
从而得到状态序列$I^*=(i_1^*,i_2^*,...,i_T^*)$.

近似算法的缺点是**不能保证预测的状态序列整体是最有可能的状态序列(不能保证是全局最优解)**,优点是计算简单.

存在的问题:上述算法得到的状态序列中有可能存在转移概率为0的相邻状态,即对某些i,j,$a_{ij}=0$时.

###维特比算法(动态规划)

**维比特算法是使用动态规划求概率最大路径(最优路径)**.这时一条路径对应着一个状态序列.

**维特比算法流程**:

输入:模型$\lambda$和观测$O=(o_1,o_2,...,o_T)$

输出:最优路径$I^*=(i_1^*,i_2^*,...,i_T^*)$

1. 初始化
   $$
   \delta_i(i) = \pi_ib_i(o_1) \ , i=1,2,..,N \\
   \psi_1(i) = 0 \ , i=1,2,...,N
   $$

2. 递推,对t=2,3,...,T
   $$
   \delta_t(i) = \max_{1 \leq j \leq N} [\delta_{t-1}(j)a_{ji}]b_i(o_t) \ , i=1,2,...,N \\
   \psi_t(i) = \arg \max_{1 \leq j \leq N} [\delta_{t-1}(j) a_{ji}] \ , i=1,2,...,N
   $$

3. 终止
   $$
   P^* = \max_{1 \leq i \leq N} \delta_T(i) \\
   i_T^* = \arg \max_{1 \leq i \leq N }[\delta_T(i)]
   $$

4. 最优路径回溯.对t=T-1,T-2,...,1
   $$
   i_t^* = \psi_{t+1}(i_{t+1}^*)
   $$


求得最优路径$I^* = (i_1^*,i_2^*,...,i_T^*)$



   