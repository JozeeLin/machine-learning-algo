# 第三章 k近邻法

k近邻法是一种基本分类与回归方法.**k近邻法的特殊情况是k=1的情形,称为最近邻算法,对于输入的实例点x,最近邻算法将训练数据集中与x最邻近点的类作为x的类**.

## k近邻模型

k近邻法使用的模型实际上对应于对特征空间的划分.模型由三个基本要素-----距离度量,k值的选择和分类决策规则决定.

### 距离度量

- 欧氏距离
- $L_p$距离
- Minkowski距离

设特征空间X是n维实数向量空间$\R^n$,特征空间中两个点$x_i,x_j$之间的$L_p$距离定义为:
$$
L_p(x_i,x_j) = (\sum_{l=1}^{n}|x_i^l-x_j^l|^p)^{\frac{1}{p}}
$$
这里$p \geq 1$.

当p=2时,称为欧氏距离;

当p=1时,称为曼哈顿距离;

当$p=\infty$时,它是各个坐标距离的最大值,即:
$$
L_\infty(x_i,x_j) = \max_l|x_i^l-x_j^l|
$$

> 这个距离公式,相当于范数,p=2时为2范数,p=1时为1范数,p为无穷时为无穷范数.

### k值选择

如果选择较小的k值,相当于用较小的邻域中的训练实例进行预测,**"学习"的近似误差会减小,只有与输入实例较劲的训练实例才会对预测结果起作用**.**缺点是"学习"的估计误差会增大,预测结果会对近邻的实例点非常敏感**.

> k值减小意味着整个模型变得复杂,容易发生过拟合.

如果选择较大的k值,相当于用较大的邻域中的训练实例进行预测,**"学习"的估计误差会减少,"学习"的近似误差会增大**.

> k值增大意味着整个模型变得简单,容易出现欠拟合.

### 分类决策规则

常用的规则为多数表决法(多数投票法).即有输入实例的k个邻近的训练实例中的多数类决定输入实例的类.

> 使误分类率最小即经验风险最小,所以多数表决规则等价于经验风险最小化.

## kd树

实现k近邻法的**主要问题是如何对训练数据进行快速k近邻搜索**.

kd树是一种对**k维空间**中的实例点进行存储以便对其进行快速检索的树形数据结构.

### 构造kd树

kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构.kd树是**二叉树**.**构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分,构成一系列的k维超矩形区域**.

构造kd树的流程:

1. 开始:构造根节点,根节点对应于包含T的k维空间的超矩形区域.

   选择$x^1$为坐标轴,以T中所有实例的$x^1$坐标的**中位数为切分点**,将根节点对应的超矩形区域切分为两个子区域.**切分由通过切分点并与坐标轴$x^1$垂直的超平面实现**.

   由根节点生成**深度为1的左右子节点**:左子节点对应坐标$x^1$小于切分点的子区域,右子节点对应于坐标$x^1$大于切分点的子区域.

2. 重复:对深度为j的结点,选择$x^l$为切分的坐标轴,$l = j(\mod k)+1$,以该结点的区域中所有实例的$x^l$坐标的中位数为切分点,将该节点对应的超矩形区域切分为两个子区域.切分由通过切分点并与坐标轴$x^l$垂直是超平面实现.

   由该节点生成深度为j+1的左右子节点:

   将落在切分超平面上的实例点保存在该结点.

3. 直到两个子区域没有实力存在时停止.从而形成kd树的区域划分.

### 用kd树的最近邻搜索

1. 在kd树中找到包含目标点的叶节点:从根节点出发,**递归地向下访问kd树**.若目标点x当前维的坐标小于切分点的坐标,则移动到左子节点,否则移动到右子节点.直到子节点为叶节点位置

2. 以此叶节点为"当前最近点"

3. **递归地向上回退**,在每个节点进行以下操作

   1. 如果该结点保存的实例点比当前最近点距离目标点更近,则以该实例点为"当前最近点".(**当前最近点更新规则**)

   2. (**同时还要检查兄弟分支**)当前最近点一定存在于该结点一个子节点对应的区域.**检查该子节点的父节点的另一子节点对应的区域是否有更近的点**.具体地,检查另一子节点对应的区域是否与以目标点为球心,以目标点与"当前最近点"间的距离为半径的**超球体相交**.

      如果**相交**,可能**在另一个子节点对应的区域内存在距目标点更近的点**,移动到另一个子节点.接着,递归地进行最近邻搜索.

      如果**不相交**,向上回退

4. 当回退到根节点时,搜索结束.最后的"当前最近点"即为x的最近邻点.

如果实例点是随机分布的,kd树搜索的平均计算复杂度为O(logN),这里的N为训练实例数.**kd树更适用于训练实例数远大于空间维数时的k近邻搜索**.



