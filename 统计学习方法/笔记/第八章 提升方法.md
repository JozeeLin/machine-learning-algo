# 第八章 提升方法

提升方法是一种常用的**统计学习方法**.在分类问题中,它通过改变训练样本的权重,学习**多个分类器**,并将这些分类器进行**线性组合**,**提高分类的性能**.

提升方法代表性的**提升算法AdaBoost**(请参阅根目录下的Adaboost子目录).但是这里也会通过训练误差分析探讨Adaboost为什么能够提高学习精度;并且从**前向分步加法模型**的角度解释Adaboost.

最后,还会介绍**提升树**.

## Adaboost算法

### 提升方法的基本思路

在概率近似正确(probably approximately correct,PAC)学习的框架中,一个概念(一个类),如果**存在一个多项式的学习算法**能够学习它,并且**正确率很高**,那么就称这个概念是**强可学习的**;一个概念,如果**存在一个多项式的学习算法**能够学习它,学习的**正确率仅比随机猜测略好**,就称这个概念是**弱可学习的**.在PAC学习的框架下,一个概念是强可学习的充分必要条件为这个概念是弱可学习的.

对于提升方法来说,有两个问题需要回答:

- 在每一轮如何改变训练数据的权值或概率分布;
- 如何将弱分类器组合成一个强分类器;

Adaboost是通过以下措施来解决以上问题的:

- 提高那些被前一轮弱分类器错误分类样本的权值,而降低那些被正确分类样本的权值.这样一来,那些没有得到正确分类的数据,由于权值加大而受到后一轮的弱分类器的更大关注.(**这个过程相当于把训练样本通过一系列的弱分类器"分而治之"**)
- Adaboost采取加权多数表决的方法.具体的,加大分类误差率小的弱分类器的权值,使其在表决中起较大的作用,减小分类误差率大的弱分类器的权值,使其在表决中起较小的作用.

### Adaboost算法流程

参阅Adaboost目录中的笔记内容.

### 个人理解总结

> 对于Adaboost算法,很类似于决策树,但是分割数据集的规则是误差率,而不是信息熵.而且Adaboost这里还引入了带有权重的弱分类器.而这个弱分类器也即是相当于(子)数据集分割规则的抽象.**更多请参阅<统计学习方法>P140中的例8.1**.

## Adaboost算法的解释

Adaboost算法是模型为**加法模型**,损失函数为**指数函数**,学习算法为**前向分布算法**时的二分类学习方法.

### 前向分步算法

考虑加法模型:
$$
f(x) = \sum_{m=1}^M \beta_mb(x;\gamma_m)
$$
其中,$b(x;\gamma_m)$为基函数,$\gamma_m$为基函数的参数,$\beta_m$为基函数的系数.

在给定训练数据及损失函数L(y,f(x))的条件下**,学习加法模型f(x)成为经验风险极小化即损失函数极小化问题**:
$$
\min_{\beta_m,\gamma_m} \sum_{i=1}^N L(y_i,\sum_{m=1}^M \beta_m b(x_i;\gamma_m)) \tag{1}
$$
前向分步算法的思想是:因为学习的是加法模型,如果能够从前向后,**每一步只学习一个基函数及其系数**,逐步逼近优化目标函数(1),那么就可以简化优化的复杂度.具体地,每步只需优化如下损失函数:
$$
\min_{\beta,\gamma} \sum_{i=1}^N L(y_i,\beta b(x_i;\gamma)) \tag{2}
$$
**前向分步算法的流程:**

输入:训练数据集T,损失函数$L(y,f(x))$,基函数集$\{b(x;\gamma\}$(**相当于Adaboost算法中的弱分类器**)

输出:加法模型f(x)

1. 初始化$f_0(x)=0$

2. 对m=1,2,...,M

   1. 极小化损失函数
      $$
      (\beta_m,\gamma_m) = \arg \min_{\beta,\gamma} \sum_{i=1}^N L(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))
      $$
      得到参数$\beta_m,\gamma_m$

   2. 更新
      $$
      f_m(x) = f_{m-1}(x) + \beta_mb(x;\gamma_m)
      $$

3. 得到加法模型
   $$
   f(x) = f_M(x) = \sum_{m=1}^M \beta_mb(x;\gamma_m)
   $$



这样,前向分布算法将同时求解从m=1到M所有参数$\beta_m,\gamma_m$的优化问题简化为逐次求解各个$\beta_m,\gamma_m$的优化问题.

### 前向分布算法与Adaboost

前向分布算法与Adaboost之间的联系,请参阅<统计学习方法>P145

##提升树

请参阅boosting tree子目录中的笔记.