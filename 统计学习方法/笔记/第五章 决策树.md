# 第五章 决策树

决策树是一种基本的分类与回归方法.本章主要讨论分类决策树.决策树模型,在分类问题中,可以认为是**if-then规则的集合**,也可以认为是**定义在特这空间与类空间上的条件概率分布**.决策树学习通常包括3个步骤:**特征选择,决策树生成,决策树剪枝**.

决策树有三种实现算法:ID3,C4.5,CART.

## 决策树模型与学习

分类决策树模型是一种描述对实例进行分类的**树形结构**.决策树由结点(node)和有向边组成.结点有两种类型:内部结点和叶节点.**内部结点表示一个特征或属性,叶节点表示一个类别**.

### 决策树与if-then规则

将决策树转换成if-then规则的过程:由决策树的根节点到叶节点的**每一条路径构建一条规则**;路径上**内部结点的特征对应着规则的条件**,而叶节点的**类别**对应着规则的**结论**.同时规则之间是**互斥且完备**的.

### 决策树与条件概率分布

决策树表示**给定特征条件下类的条件概率分布**.这一条件概率分布定义在**特征空间的一个划分上**.将特征空间划分为互不相交的单元或区域,并**在每个单元定义一个类的概率分布就构成了一个条件概率分布**(一条路径对应一个单元,也就是对应一个条件概率分布).

## 特征选择

特征选择是决定用哪个特征来划分特征空间.

### 熵

熵表示随机变量不确定性的度量.设X是一个取有限个值的离散随机变量,其概率分布为:
$$
P(X=x_i) = p_i , i=1,2,...,n
$$
则随机变量X的熵定义为:
$$
H(X) = -\sum_{i=1}^{n}p_i \log p_i
$$
由于熵只依赖于X的分布,所以将X的熵记作H(p),即:
$$
H(p) = -\sum_{i=1}^n p_i \log p_i
$$
熵越大,随机变量的不确定性就越大,则满足:
$$
0 \leq H(p) \leq \log n
$$

### 条件熵

条件熵$H(Y|X)$表示在已知给定随机变量X的条件下随机变量Y的不确定性.给定随机变量X的条件下随机变量Y的条件熵H(Y|X)定义为:
$$
H(Y|X) = \sum_{i=1}^{n}p_iH(Y|X=x_i) , p_i = P(X=x_i) , i=1,2,...,n
$$
条件熵表示给定X的条件下Y的条件概率分布的熵对X分布的数学期望.

### 信息增益

信息增益表示得知特征X的信息而使得类Y的信息的不确定性减少的程度.

特征A对训练数据集D的信息增益g(D,A),定义为集合D的经验熵H(D)与给定特征A的条件下,D的经验条件熵H(D|A)之差,即:
$$
g(D,A) = H(D) - H(D|A)
$$
**一般地,熵与条件熵之差称为互信息(mutual information)**.决策树学习中的信息增益等价于训练数据集中的类与特征的互信息.

**信息增益大的特征具有更强的分类能力**.

> 缺点:以信息增益作为划分训练数据集的准则,存在偏向于选择取值较多的特征的问题.

### 信息增益比

特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益g(D,A)与训练数据集D关于特征A的值的熵$H_A(D)$之比,即:
$$
g_R(D,A) = \frac{g(D,A)}{H_A(D)}
$$
其中,$H_A(D) = -\sum_{i=1}^{n}\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}$

## 决策树的生成

### ID3算法(信息增益)

ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征,递归地构建决策树.**ID3相当于用极大似然法进行概率模型的选择**.(因为前面提到了每个单元都是对应一个条件概率分布)

**ID3算法流程:**

输入:训练数据集D,特征集A,阈值$\epsilon$

输出:决策树T

1.若D中所有实例属于同一类$C_k$,则T为单结点树,并将类$C_k$作为该结点的类标记,返回T;(**终止条件**)

2.若A=$\varnothing$,则T为单结点树,并将D中实例数最大的类$C_k$作为该结点的类标记,返回T; (**终止条件**)

3.否则,计算A中各特征对D的信息增益,选择信息增益最大的特征$A_g$

4.如果$A_g$的信息增益小于阈值$\epsilon$ ,则置T为单结点树,并将D中实例数最大的类$C_k$作为该结点的类标记,返回T;

5.否则,对$A_g$的每一可能值$a_i$,依$A_g=a_i$将D分割为若干非空子集$D_i$,将$D_i$中实例数最大的类作为标记,构建子结点,由结点及其子结点构成树T,返回T

6.对第i个子结点,以$D_i$为训练集,以A-{$A_g$}为特征集,递归地调用步骤1~步骤5,得到子树$T_i$,返回$T_i$

> ID3算法只有树的生成,所以该算法生成的树容易产生过拟合.

### C4.5的生成算法(信息增益比)

**C4.5算法流程:**

输入:训练数据集D,特征集A,阈值$\epsilon$

输出:决策树T

1.如果D中所有实例属于同一类$C_k$,则置T为单结点树,并将$C_k$作为该结点的类,返回T

2.如果A=$\varnothing$,则置T为单结点树,并将D中实例数最大的类$C_k$作为该结点的类,返回T

3.否则,计算A中各特征对D的信息增益比,选择信息增益比最大的特征$A_g$

4.如果$A_g$的信息增益比小于阈值$\epsilon$,则置T为单结点树,并将D中实例数最大的类$C_k$作为该结点的类,返回T

5.否则,对$A_g$的每一可能值$a_i$,依$A_g=a_i$将D分割为子集若干非空$D_i$,将$D_i$中实例数最大的类作为标记,构建子结点,由结点及其子结点构成树T,返回T.

6.对结点i,以$D_i$为训练集,以A-{$A_g$}为特征集,递归地调用步骤1~步骤5,得到子树$T_i$,返回$T_i$

## 决策树的剪枝

**决策树生成算法递归地产生决策树**.以上决策树的生成算法很容易产生过拟合.解决这个问题的办法是考虑决策树的复杂度,对已生成的决策树进行简化.**决策树的简化过程称为剪枝**.

**决策树剪枝往往通过极小化决策树整体的损失函数或代价函数来实现**.

设树T的结点个数为|T|,t是树T的叶结点,该叶结点有$N_t$个样本点,其中k类的样本点有$N_{tk}$个,$k=1,2,...,K,H_t(T)$为叶结点t上的经验熵,$\alpha \geq 0$为参数,则决策树学习的损失函数可以定义为:
$$
C_\alpha(T) = \sum_{t=1}^{|T|}N_tH_t(T) +\alpha|T|
$$
其中经验熵为:
$$
H_t(T) = - \sum_k \frac{N_{tk}}{N_t}\log \frac{N_{tk}}{N_t}
$$
将经验熵公式代入损失函数式子中第一项,可得:
$$
C(T) = \sum_{t=1}^{|T|}N_tH_t(T) = - \sum_{t=1}^{|T|}\sum_{k=1}^{K} N_{tk} \log \frac{N_{tk}}{N_t}
$$
这时损失函数可以表示成:
$$
C_\alpha(T) = C(T) + \alpha|T|
$$
$C(T)$表示模型对训练数据的预测误差,即模型与训练数据的拟合程度,|T|表示模型复杂度,参数$\alpha \geq 0$控制两者之间的影响.较大的$\alpha$促使选择简单的模型,较小的$\alpha$促使选择较复杂的模型.$\alpha$为0意味着只考虑模型与训练数据之间的拟合程度,不考虑模型复杂度.

**剪枝,就是当$\alpha$确定时,选择损失函数最小的模型**.

> **损失函数的极小化**等价于**正则化的极大似然估计**.所以,利用损失函数最小原则进行剪枝就是用正则化的最大似然估计进行**模型选择**.

**剪枝算法流程:**

1. 计算每个节点的经验熵

2. 递归地从树的叶节点向上回缩

   设一组叶节点回缩到其父节点之前和之后的整体树分别为$T_B$和$T_A$,其对应的损失函数值分别是$C_\alpha(T_B)$与$C_\alpha(T_A)$,如果:

$$
C_\alpha(T_A) \leq C_\alpha(T_B)
$$

​	则进行剪枝,即将父节点变为新的叶节点.

3. 返回2,直到不能继续位置,得到损失函数最小的子树$T_\alpha$

> 决策树的剪枝算法可以由一种**动态规划**的算法实现.

## CART算法

分类与回归树(classification and regression tree, CART)模型.CART是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法(此条件概率分布为求解目标).

CART算法由以下两步组成:

1. 决策树生成:基于训练数据集生成决策树,生成的决策树要尽量大;
2. 决策树剪枝:用验证数据集对已生成的树进行剪枝并选择最优子树,这时用损失函数最小作为剪枝的标准.

### CART生成

决策树的生成就是递归的构建二叉决策树的过程,**对回归树用平方误差最小化准则,对分类树用基尼指数最小化准则,进行特征选择,生成二叉树**.

### 1.回归树的生成算法流程

输入:训练数据集D

输出:回归树$f(x)$

在训练数据集所在的输入空间中,递归的将每个区域划分为两个子区域并决定每个子区域熵的输出值,构建二叉决策树:

- 选择最优切分变量j与切分点s,求解
  $$
  \min_{j,s} [\min_{c_1} \sum_{x_i \in R_1(j,s)}(y_i-c_1)^2+\min_{c_2} \sum_{x_i \in R_2(j,s)}(y_i-c_2)^2]
  $$
  遍历变量j,对固定的切分变量j扫描切分点s,选择使上式达到最小值的(j,s)

- 用选定的对(j,s)划分区域并决定相应的输出值:
  $$
  R_1(j,s) = \{x|x^j \leq s \}, R_2(j,s)={x|x^j > s} \\
  \hat{c}_m = \frac{1}{N_m} \sum_{x_i \in R_m(j,s)} y_i, x \in R_m, m=1,2
  $$

- 继续对两个子区域调用步骤1,2,直至满足停止条件

- 将输入空间划分为M个区域$R_1,R_2,...,R_M$,生成决策树:
  $$
  f(x) = \sum_{m=1}^{M}\hat{c}_mI(x \in R_m)
  $$


### 基尼指数

分类问题中,假设有K个类,样本点属于第k类的概率为$p_k$,则概率分布的基尼指数定义为:
$$
\rm{Gini}(p) = \sum_{k=1}^K p_k(1-p_k) = 1-\sum_{k=1}^{K}p_k^2
$$
对于给定的样本集合D,其基尼指数为:
$$
\rm{Gini}(D) = 1-\sum_{k=1}^K (\frac{|C_k|}{|D|})^2
$$
这里,$C_k$是D中属于第k类的样本子集,K是类的个数.

如果样本集合D根据特征A是否取某一可能值a,被分割成$D_1$和$D_2$两部分,即:
$$
D_1 = \{(x,y) \in D | A(x)=a\}, D_2=D-D_1
$$
则在特征A的条件下,集合D的基尼指数定义为:
$$
\rm{Gini}(D,A) = \frac{|D_1|}{|D|}\rm{Gini}(D_1)+\frac{|D_2|}{|D|}\rm{Gini}(D_2)
$$
基尼指数Gini(D)表示集合D的不确定性,基尼指数Gini(D,A)表示经A=a分割后集合D的不确定性.基尼指数值越大,样本集合的不确定性也就越大,这一点与熵相似.



###2.分类树的生成算法流程

输入:训练数据集D,停止计算的条件

输出:CART决策树

1.设节点的训练数据集为D,计算现有特征对数据集的基尼指数.次数,对每一个特征A,对其可能取的每个值a,根据样本点对A=a的测试为"是"或"否"将D分割成$D_1$和$D_2$两部分,计算A=a时的基尼指数.

2.在所有可能的特征A以及它们所有可能的切分点a中,选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点.依最优特征与最优切分点,从现结点生成两个子结点,将训练数据集依特征分配到两个子结点中去.

3.对两个子结点递归地调用1,2,直至满足停止条件

4.生成CART决策树

算法停止计算的条件是结点中的样本个数小于预定阈值,或样本集的基尼指数小于预定阈值(样本基本属于同一类),或者没有更多特征.

### CART剪枝

剪枝算法流程:

1.设k=0,T=$T_0$

2.设$\alpha=+\infty$

3.自下而上地对各内部结点t计算$C(T_t)$,$|T_t|$以及
$$
g(t) = \frac{C(t) - C(T_t)}{|T_t|-1}
$$
这里,$T_t$表示以t为根结点的子树,$C_(T_t)$是对训练数据的预测误差,$|T_t|$是$T_t$的叶节点个数.

4.对$g(t) = \alpha$ 的内部结点t进行剪枝,并对叶节点t以多数投票法决定其类别,得到树T

5.设k=k+1,$\alpha_k = \alpha, T_k = T$

6.如果$T_k$不是由根结点及两个叶结点构成的树,则回到步骤3,否则令$T_k=T_n$

7.采用交叉验证法在子树序列$T_0,T_1,...,T_n$中选取最优子树$T_\alpha$

