# 第二章 感知机

感知机属于**判别模型**,旨在求出将训练数据集进行**线性划分**的分离超平面.**损失函数是基于误分类数提出的**,通过利用**梯度下降法**对损失函数进行极小化.感知机分为原始形式和对偶形式两种表示形式.

## 感知机模型

输入为实例的特征向量,输出为实例的类别.输入空间到输出空间之间的映射关系为:
$$
f(x) = \rm{sign}(WX+b)
$$
W为权重参数,b为偏置,sign是符号函数,X为实例的特征向量.

线性方程$WX+b=0$,对应着特征空间$\R^n$中一个超平面S,其中w是超平面的法向量,b是超平面的截距.

## 感知机学习策略(建模过程)

感知机给出一个假设前提是数据集线性可分的.基于这个假设,感知机学习的目标是求得一个能够将数据集中的正负实例正确分开的分离超平面.

需要制定一个学习策略求出这个分离超平面,感知机采用的损失函数是求误分类点到超平面S的总距离.

首先,输入空间中任一点$x_0$到超平面S的距离:
$$
\frac{1}{\|W\|}|WX_0+b|
$$
其次,对于误分类点来说:
$$
-y_i(WX_i+b)>0
$$
所以,误分类点到超平面S的距离,变形为:
$$
-\frac{1}{\|W\|}y_i(WX_i+b)
$$
那么,所有误分类点到超平面S的距离之和就表示成:
$$
-\frac{1}{\|W\|}\sum{y_i(WX_i+b)}
$$
上式不考虑$\frac{1}{\|W\|}$即为感知机的损失函数:
$$
L(W,b) = -\sum{y_i(WX_i+b)}
$$
损失函数是非负的,当误分类点的集合为空,那么损失函数值为0,否则损失函数值大于0.而误分类点越少,损失函数值也就越小.

## 感知机学习算法(求解过程)

感知机学习问题转化为求解损失函数是的最优化问题.求解问题的方法是使用**随机梯度下降法**.

### 原始形式

$$
\min_{w,b}L(W,b) = -\sum{y_i(WX_i+b)}
$$

损失函数梯度为:
$$
\nabla_WL(W,b) = - \sum{y_iX_i} \\
\nabla_bL(W,b) = -\sum{y_i}
$$
随机选取一个误分类点,对W,b参数进行更新:
$$
W \leftarrow W+\eta y_i X_i \\
b \leftarrow b+\eta y_i
$$
原始形式求解过程:

1. 选取初值$W_0,b_0$

2. 在训练集中选取数据($x_i,y_i$)

3. 如果$y_i(WX_i+b) \leq 0$,继续更新W,b参数:
   $$
   W \leftarrow W+\eta y_i X_i \\
   b \leftarrow b+\eta y_i
   $$

4. 转至2,直至训练集中没有误分类点.

> 感知机学习算法有多个解.它会因为采取不同的初值,以及误分类点的选取顺序不同,解也会不同.

### 算法收敛性

> (**个人总结**)算法收敛性的关键在于证明**一定存在一个超平面**可以把数据集中的正负样本正确分开,或者证明**损失函数是有界**的,或者**算法学习的迭代次数是有限**的.

### 对偶形式

对偶形式的基本想法是:将W和b表示为实例$X_i$和标签$y_i$的线性组合的形式,通过求解其系数而求得w和b.

原始形式中W和b的更新规则为:
$$
W \leftarrow W+\eta y_i X_i \\
b \leftarrow b+\eta y_i
$$
转变成:
$$
W = \sum_{i=1}^{N} \alpha_iy_iX_i \\
b = \sum_{i=1}^{N} \alpha_i y_i
$$
这里的$\alpha_i=n_i \eta$ ,$n_i$表示第i个实例点被错误分类的次数.

对偶形式的求解过程:

1. $\alpha_i \leftarrow 0,b \leftarrow 0$

2. 在训练集中选取数据$(X_i,y_i)$

3. 如果$y_i(\sum_{j=1}^{N}\alpha_j y_jX_j X_i+b) \leq 0$
   $$
   \alpha_i \leftarrow \alpha_i+\eta \\
   b \leftarrow b + \eta y_i
   $$

4. 转至2直到没有误分类数据

> 为了方便,预先将训练集中实例间的内积计算出来,并以矩阵的形式存储.这个矩阵就是所谓的**Gram矩阵**.

