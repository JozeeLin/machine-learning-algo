## 罗森布拉特感知器(属于判别模型)

> 感知器是二类分类的先行分类模型。旨在求出训练数据进行线性划分的分离超平面。是神经网络和支持向量机的基础。
>
> **说明**：以下的W,X均参考假设函数中的表示方式，而非实际的样本特征向量X。下标表示的都是样本索引号，非特征索引号。使用小括号引起来的上标表示特征索引号。大写字母表示向量，小写字母表示标量。



### 原理

1. 假设函数为$z = W^TX $ ， $ W=[w_0,w_1,w_2,...,w_m] \ X=[1,x_1,x_2,...,x_m]$ ($-w_0表示阈值也称为偏置bias$)

   > 几何解释：就是求出函数**$z$**对应的超平面S，其中，$[w_1,w_2,...w_m]$向量表示超平面的法向量，$w_0$ 表示超平面的截距。

2. 激励函数（也叫符号函数）为$\phi(z)$，如果$z>=0$，则$\phi(z)=1$，否则为-1。

3. 损失函数

   > 损失函数定义为误分类点的总数。根据假设函数以及激励函数的定义，误分类的条件为：当$y_i$为1时，$z_i$小于0，或$y_i$为-1时，$z_i$大于0。因此，误分类点满足不等式$y_i(W \bullet X_i) = y_iz_i \lt 0$。那么可以使用$J(W) = -\sum_ \limits{X_i} y_i(W \bullet X_i) \ (J(W) \geq 0)$ 来表示所有的误分类样本。通过求解$\min J(W)$就可以找到成功划分样本的超平面。

   实际编程中，对误分类样本满足的条件修正为$J(W) = \frac{1}{2}(y_i - z_i)^2 \neq 0$，梯度为$(y_i - z_i)X_i$，所以正确分类的时候，权重增量$\Delta W$始终为0，而误分类时，权重增量始终大于0，则在代码实现的时候，无论是否正确分类，始终执行$W := W + \Delta W_i$。

3. 感知器的学习算法步骤如下：

   - 将权重初始化为0或一个极小的随机数

   - 迭代所有训练样本$X_i$，执行如下操作：

     - 计算输出值$\hat{y}$（也就是激励函数返回的值）
     - 更新权重

     $$
     w^{(j)} := w^{(j)}+\Delta w^{(j)}  \\
     \Delta w^{(j)} = \eta(y_i-\hat{y_i})x_i^{(j)} \ ， （\eta为学习速率）
     $$

     所以预测错误的情况下，会趋向正确的方向(算法收敛的方向)。

     > 注意：感知器收敛的前提是两个类别必须线性可分的，且学习速率足够小(如果线性可分的情况下，学习率影响不大，但是如果是非线性可分的情况下，应该足够小，这样可以更加可能找到尽可能少的误分类数的划分方式)。否则，需要设定迭代次数最大值，或设置一个允许错误分类样本数量的阈值——否则，感知器训练会一直更新权值。

4. 综述：输入样本，计算出样本对应的假设函数的值，通过激励函数比较假设函数的值跟阈值之间大小来判断该样本是正样本还是负样本，同时把激励函数预测出来的结果跟实际观测的结果比较，如果分类错误，就对权值进行更新，否则不需要更新。一直迭代这些过程，直到错误分类样本的数量为0。所以，如果两个类别不是线性可分的话，同时不设置额外的限制条件，算法会一直迭代下去，造成堆栈溢出。




### 算法收敛性证明

> 需要证明损失函数存在最小值，以及迭代次数存在上限。

**定理：**

1. 存在满足条件$\|W_{opt}\|=1$(单位法向量)的超平面将训练数据集完全正确分开；且存在$\gamma>0$，对所有$i=1,2,...n$
   $$
   y_i(W_{opt}^TX_i) \geqslant \gamma \tag{1}
   $$



2. 令$R = \max \limits_{1\leqslant i\leqslant n} \|X_i\|$，则感知器算法在训练集上的误分类次数k满足不等式：
   $$
   k \leqslant \left ( \frac{R}{\gamma} \right)^2 \tag{2}
   $$


**证明:**

**定理1证明：**

根据前提条件数据集是线性可分的，所以必然会存在一个超平面$W_{opt}^TX=0$，则一定存在$\|W_{opt}\|=1$。则对于有限的$i=1,2,...N$，下面不等式一定成立：
$$
y_i(W_{opt}^TX_i) \gt 0
$$
所以存在：
$$
\gamma = \min_i\{y_i(W_{opt}^2X_i\}
$$
**定理2证明:** 

令$W_{k-1}$是第k个实例之前的权重向量，则第k个被误分类实例的条件是：
$$
y_k(W_{k-1}^TX_i) \leqslant 0 \tag{3}
$$
若$(X_i,y_i)$是被$W_{k-1}$误分类的数据，则权重向量会被更新：
$$
W_{k} \gets W_{k-1}+\eta y_i X_i  \\
W_k = W_{k-1}+\eta y_i X_i \tag{4}
$$
**下面延伸2个不等式：**
$$
W_k^TW_{opt} \geqslant k\eta\gamma \tag{5} \\
$$

$$
\|W_k\|^2 \leqslant k\eta^2R^2 \tag{6}
$$



由公式(4)和公式(1)得
$$
W_k^T W_{opt} = W_{k-1}^T W_{opt} + \eta y_i W_{opt}^T X_i  \geqslant W_{k-1}^T W_{opt} + \eta\gamma \tag{7}
$$
**由公式(7)递推可得到公式(5)**

由公式(4)和公式(3)得到
$$
\|W_k\|^2 = \|W_{k-1}\|^2 + 2\eta y_i W_{k-1}^T X_i + \eta^2\|X_i\|^2 \\
\leqslant \|W_{k-1}\|^2+\eta^2\|X_i\|^2 \\
\leqslant \|W_{k-1}\|^2+\eta^2R^2 \\  
\leqslant \|W_{k-2}\|^2+2\eta^2R^2 \leqslant ... \\
\leqslant k\eta^2R^2 \tag{8}
$$
**公式(8)为公式(6)的证明过程**

综合公式(5)和公式(6)得到：
$$
k\eta\gamma \leqslant W_k^T W_{opt} \leqslant \|W_k\|\|W_{opt}\| \leqslant \sqrt{k} \eta R \\
k^2\gamma^2 \leqslant kR^2 \\
k \leqslant \left ( \frac{R}{\gamma} \right)^2 \tag{9}
$$
**公式(2)得证**