# 随机森林

随机森林是bagging的一个扩展变体。RF在**以决策树为基学习器**构建bagging集成的基础上，进一步在决策树的训练过程中引入了**随机属性选择**。具体来说，**传统的决策树**在进行数据集划分的时候，它会**选择最优的特征**来进行数据集划分，而**在RF中**，对**基决策树的每个节点**，是先从该节点的特征集合中**随机选择一个包含k个属性的子集**，然后在从这个子集中选择一个最优属性用于划分。这里的参数k控制了随机性的引入程度：通常情况下，$k=\log_2d$，此处d为该节点全部的特征。

随机森林中基学习器的多样性不仅来自样本扰动，还来自特征的扰动，这使得最终集成的泛化性能可通过个体学习器之间的差异度的增加而进一步提升。通常情况下，随着基学习器的数目增加，随机森林通常会收敛到更低的泛化误差。同时，随机森林的**训练效率高于**标准的bagging算法。因为随机森林在每个节点上只是考察**特征的子集**，而标准的bagging算法则需要考察**所有的特征**。

## 结合策略

### 平均法

对数值型输出$h_i(x) \in \R$ 最常见的结合策略是使用平均法

- 简单平均法(sample averaging)

$$
H(x) = \frac{1}{T}\sum_{i=1}^{T}h_i(x)
$$

- 加权平均法(weighted averaging)

$$
H(x) = \sum_{i=1}^{T}w_ih_i(x)
$$

其中，$w_i$是个体学习器$h_i$的权重，通常要求$w_i \geq 0,\sum_{i=1}^{T}w_i = 1$。(Breiman在研究stacking回归时发现，必须使用非负权重才能确保集成性能由于单一最佳个体学习器，因此在集成学习中一般对学习器的权重施以非负约束)。

> 事实上，加权平均法可认为是集成学习研究的基本出发点，对给定的基学习器，**不同的集成学习方法可视为通过不同的方式来确定加权平均法中基学习器权重**。

加权平均法中的权重一般是从训练数据中学习得到的，由于训练样本通常不充分或存在噪声，这将使得学出的权重不完全可靠，容易出现过拟合的问题。一般而言，在个体学习器性能相差较大时使用加权平均法，而在个体学习器性能相近时使用简单平均法。

### 投票法

对分类任务来说，学习器$h_i$将从类别标记集合${c_1,c_2,\dots,c_N}$中预测出一个标记，最常见的结合策略是使用投票法(voting)。

- 绝对多数投票法(majority voting)：即某标记得票过半数，则预测为该标记，否则拒绝预测
- 相对多数投票法(plurality voting):即预测为得票最多的标记，若同时有多个标记获最高票，则从中随机选取一个
- 加权投票法:与加权平均法类似，

绝对多数投票法提供了"拒绝预测"选项，这在可靠性要求较高的学习任务中是一个很好的机制，但若学习任务要求必须提供预测结果，则绝对多数投票法会退化为相对多数投票法。

在现实任务中，不同类型个体学习器可能产生不同类型的输出值:

- 类标记 如取值为{0,1},使用类标记的投票亦称"硬投票"
- 类概率，如取值在[0,1],相当于对后验概率$P(c_j|x)$的一个估计。使用类概率的投票亦称"软投票"

### 学习法

当训练数据很多时，一种更为强大的结合策略是使用"学习法"，即通过另一个学习器来进行结合。**Stacking是学习法的典型代表**。这里我们把个体学习器称为初级学习器，用于结合的学习器称为次级学习器或元学习器(meta-learner)。

stacking先从初始数据集训练出初级学习器，然后"生成"一个新数据集用于训练初级学习器。