# 牛顿法

牛顿法是通过函数近似的方法，使用泰勒二阶展开多项式来近似代价函数，通过迭代逼近来求解泰勒二阶多项式梯度为0这个等式。

> 函数近似法或插值法:用某种简单的函数(凸函数)逼近本来的函数，通过求逼近函数的极小点来估计目标函数的极小点。

## 牛顿法

通过牛顿法来求函数$f(\theta)=0$对应的$\theta^{'}$点。对应于批量梯度下降的更新规则，牛顿法的参数更新规则如下:
$$
\theta :=\theta - \frac{f(\theta)}{f^{'}(\theta)}
$$
如图所示:

![d-](/home/parallels/Desktop/machine learning algo/ML_concept/image/gd-2.png)

图一表示，函数$f(\theta)$和$\theta$之间的关系。图二表示函数在$\theta^0$点处的切线，切线与$\theta$坐标的交点记为$\theta^1$，根据导数的定义，$f^{'}(\theta^0) = \frac{f(\theta^0)}{\theta^0-\theta^1}$，令$\Delta = \theta^0-\theta^1$，则有$\Delta = \frac{f(\theta^0)}{f^{'}(\theta^0)}$，$\theta^1 = \theta^0 - \frac{f(\theta^0)}{f^{'}(\theta^0)}$。由此可以得到递推公式:
$$
\theta^{n+1} = \theta^n - \frac{f(\theta^n)}{f^{'}(\theta^n)}
$$

### 应用

把牛顿法应用到求解凸函数的极值点，也就是求使得凸函数的梯度为0的点。应用牛顿法可得出以下更新规则:

- 标量的情况,令函数为$J(\theta)$，求解导数为0，即$J^{'}(\theta) = 0$。

$$
\theta^{n+1} = \theta^{n} - \frac{J^{'}(\theta^n)}{J^{''}(\theta^n)}
$$

- 向量的情况，令函数为$J(\theta)$，求解梯度为0，即$\nabla J(\theta) = 0$。

$$
\theta^{n+1} = \theta^{n} - H^{-1}\nabla J(\theta)
$$

此处的$H^{-1}$表示函数的海森矩阵的逆。

### 优缺点

- 缺点
  - 海森矩阵的计算量很大
- 优点
  - 收敛速度快(二次收敛quadratic conversions)

## 算法优化

[拟牛顿法](#拟牛顿法)

### 特征缩放

当所有的特征取值范围相差很大(数据的偏度skewness很大)的话，会影响梯度下降的收敛速度。如下图所示:

![d-](image/gd-1.png)

## 拟牛顿法

对于大数据，牛顿法中的海森矩阵计算量太大。因此引入拟牛顿法，它使用一个简单的矩阵来近似海森矩阵。