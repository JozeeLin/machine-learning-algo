# 梯度下降算法

## 举例说明

> 1. 为什么沿着梯度的反方向更新参数，可以使参数函数逐步变小，然后慢慢收敛？
> 2. 为什么学习速率太大会导致梯度下降算法无法收敛？
>
> 以下举个简单的例子说明。

另代价函数为$J(\theta) = \theta^2$，那么$\theta$的导数为$2\theta \ ， $$\theta$的梯度下降更新规则是：
$$
\theta := \theta - \eta \nabla J(\theta) \\
\theta := \theta - 2\eta \theta
$$
**令$\eta = 0.25$，$\theta=10$**

1. $\theta = 10 - 0.5*10 $，$\theta = 5$
2. $\theta = 5 - 0.5*5 $，$\theta = 2.5$
3. $\theta = 2.5 - 0.5*2.5 $，$\theta = 1.25$

从上面的推导过程可以看到$\theta$在逐步变小，慢慢趋向于0，对于凸函数，导数为0的点就是函数的最小值点。

**令$\eta = 1, \theta = 10$**

1. $\theta = 10 - 2*10 $，$\theta = -10$
2. $\theta = -10 + 2*10 $，$\theta = 10$

从上面的推导过程可以看到$\theta$的值一直在振荡，没有逐步变小，导致**算法无法收敛**。原因在于**学习速率$\eta$太大**了。

对于上面**假设的$\theta=-10$时**，上面的**推导依然成立**。