# 梯度下降算法

##批梯度下降算法

###证明

函数的泰勒级数展开：

标量：
$$
f(x+\Delta x) \approx f(x)+f^{'}(x) \Delta x+\frac{1}{2}f^{''}(x)\Delta x^2 \tag{1}
$$
令$\Delta x = -\eta f^{'}(x)$，$\eta$为学习速率。代入公式，可以得到结论$f(x+\Delta x) < f(x)$。

向量:
$$
f(X+\Delta X) \approx f(X) + g^T(X)\Delta X+\frac{1}{2}\Delta X^T H(X)\Delta X \tag{2}
$$
向量的泰勒级数展开的变量$X$和$\Delta X$都为向量。其中$g^T(X)$为函数$f$关于$X$的梯度，$H(X)$为函数$f$关于$X$的海森矩阵。同样，令$\Delta X = -\eta g^T(X)$，得到结论$f(X+\Delta X) < f(X)$。

###举例说明

> 1. 为什么沿着梯度的反方向更新参数，可以使参数函数逐步变小，然后慢慢收敛？
> 2. 为什么学习速率太大会导致梯度下降算法无法收敛？
>
> 以下举个简单的例子说明。

另代价函数为$J(\theta) = \theta^2$，那么$\theta$的导数为$2\theta \ ， $$\theta$的梯度下降更新规则是：
$$
\theta := \theta - \eta \nabla J(\theta) \\
\theta := \theta - 2\eta \theta
$$
**令$\eta = 0.25$，$\theta=10$**

1. $\theta = 10 - 0.5*10 $，$\theta = 5$
2. $\theta = 5 - 0.5*5 $，$\theta = 2.5$
3. $\theta = 2.5 - 0.5*2.5 $，$\theta = 1.25$

从上面的推导过程可以看到$\theta$在逐步变小，慢慢趋向于0，对于凸函数，导数为0的点就是函数的最小值点。

**令$\eta = 1, \theta = 10$**

1. $\theta = 10 - 2*10 $，$\theta = -10$
2. $\theta = -10 + 2*10 $，$\theta = 10$

从上面的推导过程可以看到$\theta$的值一直在振荡，没有逐步变小，导致**算法无法收敛**。原因在于**学习速率$\eta$太大**了。

对于上面**假设的$\theta=-10$时**，上面的**推导依然成立**。

## 随机梯度下降算法

