# 线性回归

## 问题建模

- 假设函数

$$
h(X) = W^TX \tag{1}
$$

- 代价函数

$$
J(W) = \frac{1}{2} \sum_\limits{i=1}^{m}(h(X^i)-y^i)^2 \tag{2}
$$

代价函数使用了均方误差来表示。均方误差对应了常用的欧几里德距离(欧式距离)。

- 目标函数

$$
\min J(W) \tag{3}
$$

基于均方误差最小化来进行模型求解的方法成为"最小二乘法"。

## 问题解决算法

- **梯度下降法**

权重更新规则：
$$
\theta_j = \theta_j - \eta \sum_\limits{i=1}^{m}(h_\theta(X^i)-y^i)x_j^i \ (j=1,2,3...n)
$$

$$
\theta_0 = \theta_0 - \eta \sum_\limits{i=1}^{m}(h_\theta(X^i)-y^i)
$$

- **正规方程法(**对于小样本来说，使用正规方程法会更快求解)

对函数$J(W)$对$W$求梯度$\frac{\partial J(W)}{\partial W}$，修改代价函数的书写格式成$J(W) =\frac{1}{2} (Y-XW)^T(Y-XW)$:

$\nabla AB = B^T$

$\nabla ABA^TC = CAB+C^TAB^T$

$tr A = tr A^T$

$tr AB = tr BA$
$$
\nabla J(W) = \nabla \frac{1}{2} (W^TX^TXW-W^TX^TY-Y^TXW+Y^TY) \\
= \frac{1}{2}(2 X^TXW-2X^TY) \\
= X^TXW-X^TY \tag{4}
$$
令$\nabla J(W) = 0$，则$X^TXW = X^TY$，即可得到$W = (X^TX)^{-1}X^TY$ 。

> 注：正规方程解法中会存在$X^TX$不可逆的问题，导致不可逆的原因有两个：
>
> 1. 存在线性相关的两个特征(取其一)，去掉多余特征(redundant)
> 2. 样本量少于特征量(删掉一些特征，或者正则化)

## 算法优化

### 特征缩放

> 确保所有特征在一个相近的范围内，它可以**使梯度下降算法更快的收敛**。这个缩放过程也是减小偏度的过程(skewness)。特征之间的偏度过大，对应的等高线图(contours)显椭圆形，而偏度越小，对应的等高线图越趋向于圆形。

- 均值归一化

### 随机下降法

### 牛顿法

### 拟牛顿法

### 过拟合和欠拟合

> 参数$\lambda$用于控制原始代价函数项和正则惩罚项之间的平衡，$\lambda$太大会使所有的参数过小，导致拟合曲线变成直线，造成欠拟合。太小的话就达不到解决过拟合的目的。

- 使用正则化来解决过拟合问题

梯度下降算法权重更新规则修正为：
$$
\theta_0 = \theta_0 -\eta \sum_\limits{i=1}^{m}(h_\theta(X^i)-y^i) \\
\theta_j = \theta_j - \eta [\sum_\limits{i=1}^{m}(h_\theta(X^i)-y^i)x_j^i + \lambda \theta_j]\\
\theta_j = \theta_j(1-\eta \lambda) - \eta \sum_\limits{i=1}^{m}(h_\theta(X^i)-y^i)x_j^i
$$
正规解方程法的解更正为:
$$
\theta = (X^TX+\lambda I)^{-1}X^Ty \ (I 是第一行为0剩下的所有行组成单位阵的矩阵)
$$

## 延伸-多项式回归

### 一元多项式回归(polynomial regression)

举例说明：

有一个数据集，包含两个特征房屋的长和房屋的宽，label为房屋的价格。通过现有的房屋长和宽，创建一个新的特征表示房屋的面积，直接使用房屋面积来建立面积和价格之间的关系。通过画出房屋面积和房价之间的三点图发现，两者之间的关系是非线性关系，需要用曲线来拟合，这时候就可以使用多项式回归了。

1. 使用长和宽来建立的线性模型为:$h_\theta(X) = \theta_0 + \theta_1x_1+\theta_2x_2$
2. 使用房屋面积建立的线性模型为$h_\theta(x) = \theta_0+\theta_1x_3$
3. 使用多项式建立非线性模型$h_\theta(x) = \theta_0+\theta_1x_3+\theta_2x_3^2$
4. 由于房价跟面试的关系是单调递增的，而二次曲线不是单调递增的。所以修正为$h_\theta(x) = \theta_0+\theta_1x_3+\theta_2x_3^2+\theta_3x_3^3$

通过以上的分析，最终假设函数变成了:

$h_\theta(x) = \theta_0+\theta_1x_3+\theta_2x_3^2+\theta_3x_3^3$

所以，我们就可以使用线性回归的求解算法来解决上面的多项式回归问题了。

### 多元多项式回归

## 延伸广义线性模型

### 对数线性回归

> 假设线性模型对应的输出是在指数尺度上变化的，那就将输出的对数作为线性模型逼近的目标，也就是求输入空间到输出空间的非线性函数映射。

1. 正常的线性模型的输入输出的关系为

$y = W^TX$

2. 对于输入和输出为非线性关系时，比如假设函数的输出是在指数尺度上变化的，那么输入和输出的关系为:

$ln(y)=W^TX$

**也就是说，需要找到一个单调可微的函数把线性模型得到的预测值和真实值建立映射关系。**