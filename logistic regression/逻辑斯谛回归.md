# 逻辑斯谛回归(判别模型)

> 逻辑斯谛回归是一种概率模型。属于对数线性模型。
>
> 定义logit函数为：$logit(p) = log \frac{p}{1-p}$ ，p表示正样本发生的概率。因为logit函数的输入值范围介于区间[0,1],所以他能将输入转换到整个实数范围内。由此可以将logit函数极为输入特征值的线性表达式：
>
> $logit(p(y=1|x))=w_0x_0+w_1x_1+...+w_mx_m=\sum_\limits{i=0}^{n}w_ix_i=W^TX$
>
> 也可以把上述的表达式称为假设函数$h_w(x)$.
>
> logit函数的反函数为logistic函数(也称为sigmoid函数)：
>
> $\phi(z)=\frac{1}{1+e^z} \ , z=W^TX$
>
> $z$也被称为净输入;sigmoid函数返回的是当前样本是正样本的概率。

##问题建模

- 假设函数(净输入)

$h_w(x) = \sum_\limits{i=0}^{n}w_ix_i=W^TX \ , (n为样本数)$

- sigmoid函数

> sigmoid函数是一个概率函数，表示净输入对应的正样本(y=1)的概率是多少。$1-\phi(z)$表示净输入对应的负样本(y=0)的概率是多少。

logistic函数(sigmoid 函数)：

$\phi(z)=\frac{1}{1+e^z} \ , z=h_w(x) \ , (0 \leq \phi(z) \leq 1)$

- 量化器

$$
\begin{equation}
y=\left\{
\begin{aligned}
1 & & 若\phi(z) \geq 0.5 \\
0 & & 其他
\end{aligned}
\right.
\end{equation}
$$

等价于
$$
\begin{equation}
y=\left\{
\begin{aligned}
1 & & 若z \geq 0.0 \\
0 & & 其他
\end{aligned}
\right.
\end{equation}
$$

- 代价函数

原始的代价函数形式为：

$J(W) =\frac{1}{2} \sum_\limits{i=1}^n (\phi(z^i)-y^i)^2$

以上函数是非凸函数，无法通过梯度下降来找到全局最小值。我们可以通过构建似然函数得到一个凸函数，同时通过最大似然估计(极大化似然函数)来求解模型参数。
$$
似然函数：L(W)=P(y|X; W) = \prod_{i=1}^nP(y^i|X^i; W) = \prod_{i=1}^n(\phi(z^i))^{y^i}(1-\phi(z^i))^{1-y^i} \tag{1}
$$

$$
对数似然函数：l(W) = log{L(W)} = \sum_\limits{i=1}^n y^i log(\phi(z^i)) + (1-y^i)log(1-\phi(z^i)) \tag{2}
$$

由于最小化目标函数就是最大化对数似然函数，所以令$J(W) =  -l(W)$，此时的代价函数为凸函数。
$$
J(W) = -\sum_\limits{i=1}^n y^i log(\phi(z^i)) + (1-y^i)log(1-\phi(z^i)) \tag{3}
$$

$$
\begin{equation}
J(\phi(z),y;W) =\left\{
\begin{aligned}
-log(\phi(z)) & & 若y=1 \\
-log(1-\phi(z)) & & 若y=0
\end{aligned}
\right.
\end{equation} \tag{4}
$$

为什么选这个函数作为代价函数？

> 通过画出$J(W)$和$\phi(z)$ 之间的曲线图可以发现，正样本被错误分类成副样本时，$J(W)$的值变得很大，同理负样本被错误分类时，$J(W)$也会变得很大。总的来说，就是样本被错误分类时，代价函数的值会变大，那么通过最小化代价函数，我们就可以减少样本的误分类数，随着算法收敛，最终会尽可能把所有样本正确分类。

- 目标函数

$\min J(W)$

## 问题解决算法

###梯度下降法

权重更新规则：

我们的目标是求得能够使$l(W)$对数似然函数最大化，所以最大似然函数的权重更新规则为：
$$
w_j := w_j + \eta \sum_\limits{i=1}^{n} (y^i - \phi(z^i))x_j^i \tag{5} \\
w_j := w_j + \Delta w_j \\
\frac {\partial{l(W)}} {\partial{w_j}} = \sum_\limits{i=1}^{n} (y^i - \phi(z^i))x_j^i \\
W := W + \Delta W \ , \Delta W = \eta \nabla l(W)
$$
由于最大化对数似然函数等价于最小化代价函数，因为$J(W)=-l(W)$。因此回归系数更新规则修正为：
$$
\Delta w_j = -\eta \frac{\partial J}{\partial w_j} = -\eta \sum_\limits{i=1}^{n} (y^i - \phi(z^i))x_j^i  \\
W := W + \Delta W  \ ，\Delta W = -\eta \nabla J(W) \tag{6}
$$

## 算法优化

### 过拟合和欠拟合

> **过拟合**表示模型有**高方差**。方差用来衡量模型**对特定样本实例预测的一致性**。原因可能是使用了过多的参数，使得模型变得过于复杂导致的。过拟合也表示模型的泛化能力差。
>
> **欠拟合**表示模型有**高偏差**。偏差用于**从总体上衡量**预测值与实际值之间的差异。原因可能是模型过于简单，无法发现样本中隐含的模式。
>
> **偏差-方差权衡**就是通过正则化调整模型的复杂度。
>
> **正则化是解决共线性(特征间高度相关)的一种很有效的方法。**
>
> **正则化的原理**是引入权重惩罚项(偏差)来对极端权重参数进行惩罚，降低模型的复杂度。最常用的正则化为L2正则化。

L2正则化，也称作L2收缩或权重衰减：
$$
\frac{\lambda}{2} \|W\|^2 = \frac{\lambda}{2}\sum_\limits{i=1}^m w_j^2 \ , (一共有m个权重)
$$
其中，$\lambda$为正则化系数，用于平衡原始代价函数与惩罚项之间的平衡，$\lambda$过大会导致权重过小，从而导致欠拟合。

综上，引入正则化的代价函数修正为：
$$
J(W) = -\sum_\limits{i=1}^n y^i log(\phi(z^i)) + (1-y^i)log(1-\phi(z^i)) + \frac{\lambda}{2} \|W\|^2 \\ \tag{7}
J(W) = C \{-\sum_\limits{i=1}^n y^i log(\phi(z^i)) + (1-y^i)log(1-\phi(z^i)) \}+ \frac{1}{2} \|W\|^2 \ , (C = \frac{1}{\lambda})
$$
更新规则修正为：
$$
\theta_0 = \theta_0 -\eta \sum_\limits{i=1}^{m}(h_\theta(X^i)-y^i) \\
\theta_j = \theta_j - \eta [\sum_\limits{i=1}^{m}(h_\theta(X^i)-y^i)x_j^i + \lambda \theta_j]\\
\theta_j = \theta_j(1-\eta \lambda) - \eta \sum_\limits{i=1}^{m}(h_\theta(X^i)-y^i)x_j^i
$$


###拟牛顿法(比梯度下降收敛速度更快)

## 逻辑斯谛分布

设X是连续随机变量，X服从逻辑斯谛分布是指X具有下列分布函数和密度函数：
$$
F(x) = P(X \leq x) = \frac{1}{1+e^{-(x-\mu)/\gamma}} \tag{8}
$$

$$
f(x) = F^{'}(x) = \frac{e^{-(x-\mu)/\gamma}}{\gamma (1+e^{-(x-\mu)/\gamma})^2} \tag{9}
$$

分布函数以点$(\mu \ , \frac{1}{2})$为对称中心，即满足：
$$
F(-x+\mu)-\frac{1}{2} = -F(x+\mu)+\frac{1}{2} \tag{10}
$$
式中的$\mu$ 为位置参数，$\gamma > 0$为形状参数。形状参数的值越小，分布函数在位置参数附近的函数值变化越快，也表示密度函数在位置参数附近的值会越大。

## 二分类逻辑斯谛回归模型

二项逻辑斯谛回归模型由以下两个条件概率分布来表示：
$$
P(y=1|X) =\frac {e^{W^TX}}{1+e^{W^TX}} \\
P(y=0 |X) = \frac{1}{1+e^{W^TX}} \tag{11}
$$
一个事件的几率(odds)是指该事件发生的概率与该事件不发生的概率的比值：$odds =\frac{p}{1-p}$，相应的对数几率(也叫logit函数)为：
$$
logit(p) = log{\frac{p}{1-p}} \tag{12}
$$
把公式(11)代入公式(12)，得：
$$
log{\frac{P(y=1|X)}{1-P(y=1|X)}} = W^TX
$$

## 多分类逻辑斯谛回归模型

参考二分类逻辑斯谛回归模型，多分类逻辑斯谛模型的条件概率分布表达式为：
$$
P(y=k|X) = \frac{e^{W^TX}}{1+\sum_\limits{k=1}^{K-1}e^{W^TX}} \ , k=1,2,...,K-1 \\
P(y=K|X) = \frac{1}{1+\sum_\limits{k=1}^{K-1}e^{W^TX}} \tag{13}
$$
多分类逻辑斯谛回归模型的参数求解，依然是使用最大化似然函数来求解。