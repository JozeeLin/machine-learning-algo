## 自适应线性神经元以及学习的收敛性

> 自适应线性神经网络(Adaptive Linear Neuron ,Adaline)是在感知器的基础上进行改进得到的。基于Adaline规则的权重更新是通过一个连续的线性激励函数来完成的,而不像感知器那样使用单位阶跃函数。线性激励函数在更新权重的同时，使用量化器对类标进行预测，量化器跟感知器的激励函数类似。

### 原理

- 假设函数为$z = W^TX $ ， $ W=[w_1,w_2,...,w_m] \ X=[x_1,x_2,...,x_m]$ 

- 激励函数为$\phi(z) = z$，此处函数$z$修正为$z = -\theta+W^TX$ ，$\theta=-x_0w_0$ ，$x_0=1 \ \theta=-w_0$。($\theta$为阈值)

- 代价函数定义为通过模型得到的输出与实际值之间的误差平方和(Sum of Squared Error, SSE) 
  $$
  J(W) = \frac{1}{2} \sum_i(y^i-\phi(z^i))^2
  $$


> 代价函数的性质：
>
> 1. 代价函数可导
> 2. 凸函数

- 目标函数为求代价函数的最小值，从而得到最小值对应的W值。(W为向量)

$$
\min J(W)
$$

### 目标函数求解过程

> 通过梯度下降法来求解目标函数。

1. 通过梯度下降，可以基于代价函数沿梯度方向做一次权重更新
   $$
   W:=W+\Delta W 
   $$

2. 权重增量定义为负梯度与学习速率$\eta$的乘积

$$
\Delta W=-\eta \Delta J(W)
$$

> 计算代价函数的梯度相当于计算代价函数相对于每个权重$w_j$的偏导$\frac{\partial{J}}{\partial{w_j}}=- \sum_i(y^i-\phi(z^i))x_j^i$。特别的$\frac{\partial{J}}{\partial{w_0}}=- \sum_i(y^i-\phi(z^i))$
>
> 注意：所有权重必须同时更新

### 算法收敛性

$\Delta W=0$的时候，当前的代价函数取到了最小值了。也就是所有的样本都被正确划分了。学习速率$\eta$和迭代次数n_iter都被称为模型的超参数。选择合适的学习速率可以快速的找到最小值，否则学习速率过大会在学习过程中跳过最小点，学习速率太小会导致算法收敛的速度太慢，效率低下。

### 算法优化

