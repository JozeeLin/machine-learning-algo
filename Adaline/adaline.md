# 自适应线性神经元以及学习的收敛性

> 自适应线性神经网络(Adaptive Linear Neuron ,Adaline)是在感知器的基础上进行改进得到的。基于Adaline规则的权重更新是通过一个连续的线性激励函数来完成的,而不像感知器那样使用单位阶跃函数。线性激励函数在更新权重的同时，使用量化器对类标进行预测，量化器跟感知器的激励函数类似。

[TOC]

## 梯度下降



### 原理

- 假设函数为$z = W^TX $ ， $ W=[w_0,w_1,w_2,...,w_m] \ X=[1,x_1,x_2,...,x_m]$ ($-w_0$ 表示阈值)

- 激励函数为$\phi(z) = z$

- 代价函数定义为通过模型得到的输出与实际值之间的误差平方和(Sum of Squared Error, SSE) 
  $$
  J(W) = \frac{1}{2} \sum_i(y^i-\phi(z^i))^2
  $$




> 代价函数的性质：
>
> 1. 代价函数可导
> 2. 凸函数

- 目标函数为求代价函数的最小值，从而得到最小值对应的W值。(W为向量)

$$
\min J(W)
$$

### 目标函数求解过程

> 通过**梯度下降**法来求解目标函数。这里是使用所有的样本来进行权重更新，所以称为**批量梯度下降(Batch Gradient Descent)**

1. 通过**梯度下降**，可以基于代价函数沿梯度方向做一次权重更新
   $$
   W:=W+\Delta W
   $$

2. 权重增量定义为负梯度与学习速率$\eta$的乘积

$$
\Delta W=-\eta \Delta J(W)
$$

> 计算代价函数的梯度相当于计算代价函数相对于每个权重$w_j$的偏导$\frac{\partial{J}}{\partial{w_j}}=- \sum_i(y^i-\phi(z^i))x_j^i$。特别的$\frac{\partial{J}}{\partial{w_0}}=- \sum_i(y^i-\phi(z^i))$
>
> 注意：所有权重必须同时更新

### 算法收敛性

$\Delta W=0$的时候，当前的代价函数取到了最小值了。也就是所有的样本都被正确划分了。学习速率$\eta$和迭代次数n_iter都被称为模型的超参数。选择合适的学习速率可以快速的找到最小值，否则学习速率过大会在学习过程中跳过最小点，学习速率太小会导致算法收敛的速度太慢，效率低下。

### 算法优化

1. 对特征值范围进行特征缩放，以优化算法的性能。梯度下降就是通过特征缩放而受益的众多算法之一。

   特征缩放方法有:

   - 标准化特征缩放。此方法可以使数据具备标准正态分布的特性：各特征值的均值为0,标准差为1。

$$
x_j^{'} = \frac{x_j-\mu_j}{\sigma_j}
$$

​	$\mu_j$ 表示特征j的所有样本平均值，$\sigma_j$表示特征j的所有样本的标准差。

​	**实验结论：通过对特征进行缩放，即使学习速率过大，也能收敛（注意：此处的样本为线性可分的）**

2. 前面批量梯度下降算法使用所有的样本来更新权重，这种方式，对于样本量很大的情况下，会大大增加计算成本和存储成本。这里使用随机梯度下降算法来替代它。

## 随机梯度下降

### 原理

> 随机梯度下降也叫做迭代梯度下降（iterative gradient descent）或者在线梯度下降（on-line gradient descent）。权重的更新规则是每次使用一个训练样本逐步更新权重。
>
> 缺点:
>
> 由于梯度的计算是基于单个训练样本来完成的，因此其误差曲线不及梯度下降的平滑，这会使随机梯度下降更容易跳过最小值点。为了是算法更好的收敛，每次迭代都要把样本数据进行打乱处理，否则可能会导致算法无法收敛。
>
> 优点：
>
> - 计算成本比批量梯度下降法低，面对海量数据时特别有效。
> - 可以将它用于在线学习。通过在线学习，当有新的数据输入是模型会被实时训练